# BU Superagent Environment Configuration
# Copy to .env and customize for your deployment

# ===== Vector Store Configuration =====
VECTOR_BACKEND=qdrant
# Supported: qdrant | chroma | faiss | weaviate | elasticsearch

# Qdrant Configuration
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=
QDRANT_PREFER_GRPC=false
QDRANT_TIMEOUT_S=30

# Chroma Configuration (legacy)
CHROMA_DIR=var/chroma/e5_1024d

# Collection Settings
VECTOR_COLLECTION=kb_chunks_de_1024d
STORE_TEXT_PAYLOAD=false
# true = store full text in vector DB (simpler, but expensive)
# false = store only vectors, use blob store for text (scalable)

# ===== Scaling Configuration =====
VECTOR_SHARDS=6
# Number of shards for horizontal scaling (Qdrant only)
# Recommendation: 1 shard per node

VECTOR_REPLICAS=2
# Replication factor for high availability (Qdrant only)
# Recommendation: 2 for production, 1 for dev/test

# Quantization (Compression)
USE_QUANTIZATION=true
QUANTIZATION_KIND=scalar
# Supported: scalar (4x compression, minimal quality loss)
#            product (16x compression, moderate quality loss)
#            binary (32x compression, high quality loss)
# Recommendation: scalar for production

# ===== Embedding Configuration =====
EMBEDDING_MODEL=intfloat/multilingual-e5-large-instruct
# Alternatives:
# - mixedbread-ai/mxbai-embed-de-large-v1 (German-specific)
# - jinaai/jina-embeddings-v2-base-de (German-specific)
# - intfloat/multilingual-e5-large (multilingual)

EMBEDDING_DEVICE=cpu
# Supported: cpu | cuda | mps
# Recommendation: cuda for production (10-50x faster)

EMBEDDING_BATCH_SIZE=512
# GPU-optimal batch size
# RTX 3090/4090: 512 (24GB VRAM)
# A100: 1024-2048 (40-80GB VRAM)
# CPU: 32-64 (memory constraints)

# ===== Work Queue Configuration =====
WORKQUEUE_BACKEND=fake
# Supported: redis | fake
# Recommendation: redis for production

# Redis Configuration
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=

# ===== Blob Storage Configuration =====
BLOBSTORE_BACKEND=fake
# Supported: minio | s3 | fake
# Recommendation: minio for self-hosted, s3 for AWS

# MinIO/S3 Configuration
MINIO_ENDPOINT=localhost:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin
MINIO_BUCKET=rag-documents
MINIO_SECURE=false
# true for HTTPS, false for HTTP (dev only)

# ===== LLM Configuration =====
LLM_BASE_URL=http://localhost:8000/v1
# vLLM, Ollama, or any OpenAI-compatible API

LLM_API_KEY=EMPTY
# Use "EMPTY" for local servers without auth

LLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
# Alternatives:
# - meta-llama/Meta-Llama-3.3-70B-Instruct (high quality)
# - mistralai/Mistral-7B-Instruct-v0.3 (fast)
# - deepseek-ai/deepseek-coder-33b-instruct (coding)

# ===== Reranker Configuration =====
RERANKER_MODEL=BAAI/bge-reranker-v2-m3
# Alternatives:
# - cross-encoder/ms-marco-MiniLM-L-6-v2 (fast, English)
# - BAAI/bge-reranker-v2-m3 (multilingual, German)

RERANKER_DEVICE=cpu
RERANKER_APPLY_SIGMOID=true
# true for BGE models (converts logits to probabilities)
# false for cross-encoder models (already probabilities)

# ===== Query Configuration (Feature Flags) =====
QUERY_USE_HYBRID=true
# Enable hybrid search (vector + lexical fusion with RRF)
# Recommendation: true for high recall

QUERY_USE_MMR=true
# Enable MMR for diversity
# Recommendation: true to avoid redundant results

QUERY_CONFIDENCE_THRESHOLD=0.25
# Minimum confidence for answering (0.0-1.0)
# Lower = more answers, higher = more reliable
# Recommendation: 0.25-0.35 for production

# ===== Telemetry Configuration =====
TELEMETRY_ENABLED=true
# Enable OpenTelemetry metrics and tracing

OTLP_ENDPOINT=
# Leave empty for console logging only
# Set to http://otel-collector:4317 for production

TELEMETRY_ENVIRONMENT=production
# Environment label for metrics (dev | staging | production)

# ===== Legacy Compatibility =====
QDRANT_HOST=localhost
# Deprecated: Use QDRANT_URL instead

QDRANT_PORT=6333
# Deprecated: Use QDRANT_URL instead
